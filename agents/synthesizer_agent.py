# agents/synthesizer_agent.py

import uuid
import logging
from typing import List, Dict, Any, Optional
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from agents.embedding_client import EmbeddingClient
from agents.codebook_repository import CodebookRepository

# Usamos el mismo logger configurado en el cliente para consistencia
logger = logging.getLogger(__name__)

class SynthesizerAgent:
    """
    Agente Sintetizador-Auditor simplificado.
    Detecta duplicados, actualiza el codebook y devuelve un mapa de traducciÃ³n.
    """
    def __init__(self, repository: CodebookRepository, client: EmbeddingClient, similarity_threshold: float = 0.90):
        logger.info("ğŸš€ Inicializando SynthesizerAgent...")
        self.repository = repository
        self.client = client
        self.similarity_threshold = similarity_threshold
        
        self.codebook = self.repository.load()
        # Asumiendo que el modelo de embedding de Google usa 768 dimensiones.
        self.embedding_dim = self.codebook.get('metadata', {}).get('embedding_dim', 768) 
        
        self._ensure_metadata_structure()
        self._rebuild_internal_caches()
        logger.info("âœ… SynthesizerAgent listo.")

    def _ensure_metadata_structure(self):
        """Asegura que la estructura de metadata exista en el codebook."""
        if 'metadata' not in self.codebook:
            self.codebook['metadata'] = {}
        
        if 'embedding_dim' not in self.codebook['metadata']:
            self.codebook['metadata']['embedding_dim'] = self.embedding_dim
            
        logger.info(f"ğŸ“Š Metadata inicializada: embedding_dim={self.codebook['metadata']['embedding_dim']}")

    def _rebuild_internal_caches(self):
        """Construye cachÃ©s en memoria para un rendimiento O(1) y O(N) rÃ¡pido."""
        logger.info("âš¡ï¸ Construyendo cachÃ©s internos para acceso rÃ¡pido...")
        self.codes_by_id: Dict[str, Dict] = {c['id']: c for c in self.codebook.get('codes', [])}
        self.label_to_id: Dict[str, str] = {c['label']: c['id'] for c in self.codebook.get('codes', [])}
        
        codes_with_embeddings = [c for c in self.codebook.get('codes', []) if 'embedding' in c and isinstance(c['embedding'], list) and len(c['embedding']) > 0]
        
        if codes_with_embeddings:
            self.ordered_code_ids = [c['id'] for c in codes_with_embeddings]
            self.embedding_matrix = np.array([c['embedding'] for c in codes_with_embeddings])
        else:
            self.ordered_code_ids = []
            self.embedding_matrix = np.empty((0, self.embedding_dim))
        logger.info(f"âš¡ï¸ CachÃ©s construidos. Matriz de embeddings tiene {self.embedding_matrix.shape[0]} vectores.")

    def process_batch(self, new_code_labels: List[str]) -> Dict[str, str]:
        """Procesa un lote de nuevas etiquetas de cÃ³digo y devuelve un mapa de traducciÃ³n."""
        
        # Inicializamos el mapa que serÃ¡ nuestro valor de retorno.
        translation_map: Dict[str, str] = {}
        
        unique_labels = {label.strip() for label in new_code_labels if isinstance(label, str) and label.strip()}
        
        codes_to_create = []
        
        for label in unique_labels:
            if label in self.label_to_id:
                unified_id = self.label_to_id[label]
                self._update_code_count(unified_id)
                # Poblamos el mapa para los duplicados exactos.
                translation_map[label] = unified_id
            else:
                codes_to_create.append(label)
        
        if codes_to_create:
            # Pasamos el mapa para que sea poblado por el mÃ©todo secuencial.
            self._process_new_codes_sequentially(codes_to_create, translation_map)

        self.repository.save(self.codebook)
        logger.info(f"âœ… Lote procesado. Codebook tiene ahora {len(self.codebook['codes'])} cÃ³digos Ãºnicos.")
        
        # Devolvemos el mapa de traducciÃ³n completo.
        return translation_map

    def _process_new_codes_sequentially(self, labels: List[str], translation_map: Dict[str, str]):
        """
        Procesa nuevos cÃ³digos uno por uno, poblando el mapa de traducciÃ³n.
        """
        logger.info(f"ğŸ§  Procesando secuencialmente {len(labels)} nuevos cÃ³digos candidatos...")
        
        if not labels:
            return

        all_embeddings = self.client.get_embeddings(labels)

        for label, embedding_list in zip(labels, all_embeddings):
            if not embedding_list:
                logger.warning(f"Se omitiÃ³ el cÃ³digo '{label}' porque no se pudo generar su embedding.")
                continue

            embedding = np.array(embedding_list)
            existing_id = self._find_semantic_duplicate(embedding)
            
            if existing_id:
                self._update_code_count(existing_id)
                # Se encontrÃ³ un duplicado semÃ¡ntico. Registramos la traducciÃ³n.
                translation_map[label] = existing_id
            else:
                new_id = f"code_{uuid.uuid4()}"
                new_code_obj = {"id": new_id, "label": label, "count": 1, "embedding": embedding_list}
                self._add_single_code_to_cache(new_code_obj, embedding)
                # Se creÃ³ un cÃ³digo nuevo. Registramos la traducciÃ³n.
                translation_map[label] = new_id
                
    def _add_single_code_to_cache(self, code: Dict, embedding: np.ndarray):
        """AÃ±ade un Ãºnico cÃ³digo nuevo a todos los cachÃ©s en memoria."""
        logger.info(f"â• AÃ±adiendo nuevo cÃ³digo Ãºnico al codebook: '{code['label']}'")
        self.codebook['codes'].append(code)
        self.codes_by_id[code['id']] = code
        self.label_to_id[code['label']] = code['id']
        self.ordered_code_ids.append(code['id'])
        
        if self.embedding_matrix.shape[0] == 0:
            self.embedding_matrix = embedding.reshape(1, -1)
        else:
            self.embedding_matrix = np.vstack([self.embedding_matrix, embedding])

    def _find_semantic_duplicate(self, new_embedding: np.ndarray) -> Optional[str]:
        """Encuentra duplicados semÃ¡nticos usando la matriz de embeddings actual."""
        if self.embedding_matrix.shape[0] == 0:
            return None
        
        similarities = cosine_similarity(new_embedding.reshape(1, -1), self.embedding_matrix)[0]
        max_similarity_idx = np.argmax(similarities)
        
        if similarities[max_similarity_idx] >= self.similarity_threshold:
            return self.ordered_code_ids[max_similarity_idx]
        return None

    def _update_code_count(self, code_id: str):
        """Incrementa el contador de un cÃ³digo existente."""
        if code_id in self.codes_by_id:
            logger.info(f"ğŸ”„ CÃ³digo duplicado ('{self.codes_by_id[code_id]['label']}') encontrado. Incrementando contador para ID: {code_id}")
            self.codes_by_id[code_id]['count'] += 1
        else:
            logger.warning(f"Advertencia de integridad: Se intentÃ³ actualizar el ID '{code_id}' no encontrado en cachÃ©.")
