import google.generativeai as genai
import json
from typing import Dict, Any, List, Union, Optional
from pydantic import ValidationError
import logging
from tenacity import retry, stop_after_attempt, wait_exponential

from models.data_models import CodingResult
from services.llm_service import LLMService
from utils.file_utils import extract_json_from_text

class CoderAgent:
    """
    Un agente de IA especializado en la codificaci贸n abierta de la Teor铆a Fundamentada.
    Este agente recibe un insight, lo formatea dentro de un prompt muy espec铆fico,
    y utiliza un modelo generativo para generar c贸digos conceptuales.
    """

    # PLANTILLA CORREGIDA:
    # Se han duplicado todas las llaves de los ejemplos JSON ( {{ y }} )
    # para que el m茅todo .format() de Python las ignore. La 煤nica llave
    # simple es la de nuestro marcador de posici贸n {json_input}.
    PROMPT_TEMPLATE = """
# PROMPT DE CODI-ACCION: CODIFICACIN ABIERTA

<persona_y_mision>
Tu 煤nica funci贸n es ser un Analista de Datos Cualitativos. Tu especialidad es la Codificaci贸n Abierta de la Teor铆a Fundamentada.
Tu misi贸n es identificar y nombrar las ACCIONES o PROCESOS en un fragmento de texto.
NO resumas. NO interpretes. SOLO codifica la acci贸n, evitando repetir c贸digos muy similares.
</persona_y_mision>

<reglas_de_codificacion_inquebrantables>
### 1. Formato del C贸digo: 隆OBLIGATORIO!
- **ESTRUCTURA EXACTA:** Cada c贸digo DEBE empezar con un verbo en gerundio (-ando, -endo, -iendo).
- **EJEMPLO:** `Buscando reconocimiento`, `Reduciendo la fricci贸n`.
- **IMPORTANTE:** Si un c贸digo no empieza con gerundio, la respuesta es INCORRECTA.

### 2. Errores Cr铆ticos que DEBES Evitar:
- ** NO uses temas o categor铆as:** Incorrecto: "salario bajo". Correcto: "Percibiendo inequidad salarial".
- ** NO uses sentimientos aislados:** Incorrecto: "tristeza". Correcto: "Experimentando desmotivaci贸n".

### 3. C贸digos "In Vivo":
- Si el texto contiene una frase literal muy potente, 煤sala.
- **Formato:** `[in vivo] "frase literal exacta"`.
- **ATENCIN AL ESCAPE:** Si la frase literal contiene comillas dobles ("), es OBLIGATORIO escaparlas con una doble barra invertida (`\\"`).
- **Ejemplo con escape:** `"[in vivo] \\"la frase textual con comillas va aqu铆\\""`.
</reglas_de_codificacion_inquebrantables>

<tarea_especifica_json>
Tu tarea es completar un objeto JSON. Te proporcionar茅 un JSON con las claves "id_fragmento" y "fragmento_original" ya rellenadas.
T煤 NO DEBES modificar esos valores. Tu 煤nico trabajo es generar la lista de strings para la clave "codigos_abiertos", siguiendo las reglas de codificaci贸n.
</tarea_especifica_json>

<ejemplo_maestro>
<input_del_usuario>
```json
{{
  "id_fragmento": "ejemplo_01",
  "fragmento_original": "Mi jefa es de esas personas que te dice 'hazlo como creas', pero despu茅s, si no es como ella lo ten铆a en la cabeza, te cae la bronca. Al final, uno termina 'adivinando el futuro' para no meter la pata.",
  "codigos_abiertos": []
}}
```
</input_del_usuario>
<output_del_modelo>
```json
{{
  "id_fragmento": "ejemplo_01",
  "fragmento_original": "Mi jefa es de esas personas que te dice 'hazlo como creas', pero despu茅s, si no es como ella lo ten铆a en la cabeza, te cae la bronca. Al final, uno termina 'adivinando el futuro' para no meter la pata.",
  "codigos_abiertos": [
    "Navegando expectativas no declaradas",
    "Intentando prevenir cr铆ticas futuras",
    "[in vivo] \"adivinando el futuro\""
  ]
}}
```
</output_del_modelo>

<verificacion_final_interna>
VERIFICAR GERUNDIO: 驴Todos los c贸digos que he generado empiezan con un verbo en gerundio (o son [in vivo])?
VERIFICAR DATOS DE ENTRADA: 驴He copiado id_fragmento y fragmento_original exactamente sin modificarlos?
VERIFICAR FORMATO FINAL: 驴La salida que voy a producir es un 煤nico objeto JSON v谩lido y completo?
</verificacion_final_interna>

<input_del_usuario>
```json
{json_input}
```
</input_del_usuario>
<output_del_modelo>
"""

    def __init__(self, llm_service: LLMService, prompt_template_path: str = 'prompts/open_coding.md'):
        """
        Inicializa el CoderAgent con sus dependencias.

        Args:
            llm_service: Una instancia de un servicio para interactuar con el LLM.
            prompt_template_path: La ruta al archivo de plantilla del prompt.
        """
        self.logger = logging.getLogger(__name__)
        self.llm_service = llm_service
        self.prompt_template_path = prompt_template_path
        self._prompt_template: Optional[str] = None # Para carga perezosa (lazy loading)

    def _load_prompt_template(self) -> str:
        """
        Carga la plantilla de prompt desde el archivo .md, usando un sistema de cach茅 simple.
        """
        if self._prompt_template is None:
            try:
                with open(self.prompt_template_path, 'r', encoding='utf-8') as f:
                    self._prompt_template = f.read()
            except FileNotFoundError:
                self.logger.error(f"Archivo de plantilla de prompt no encontrado en: {self.prompt_template_path}")
                raise
        return self._prompt_template

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
    def generate_codes(self, insight: Dict[str, Any]) -> CodingResult:
        """
        Genera c贸digos para un 煤nico insight y devuelve un objeto validado.

        Args:
            insight: Un diccionario que representa un insight, debe contener 'id' y 'text'.

        Returns:
            Un objeto CodingResult validado.
        """
        # 1. Transformar el insight al formato esperado por el prompt.
        # El prompt espera 'id_fragmento' y 'fragmento_original'.
        insight_for_prompt = {
            "id_fragmento": insight.get("id"),
            "fragmento_original": insight.get("text"),
            "codigos_abiertos": []
        }
        json_input_str = json.dumps(insight_for_prompt, indent=2, ensure_ascii=False)

        # 2. Cargar la plantilla y rellenarla con el string JSON.
        prompt_template = self._load_prompt_template()
        final_prompt = prompt_template.format(json_input=json_input_str)
        
        try:
            # 3. Llamar a la API a trav茅s del servicio centralizado.
            llm_response_text = self.llm_service.invoke_llm(final_prompt)
            
            # 4. Usar nuestra utilidad experta para extraer y parsear el JSON de la respuesta.
            if not llm_response_text:
                raise ValueError("La respuesta del LLM estaba vac铆a.")

            json_output = extract_json_from_text(llm_response_text)
            
            if not json_output:
                # Si la extracci贸n falla, usamos print() para una depuraci贸n a prueba de fallos.
                print("\n" + "="*80)
                print(f"DEBUG: REPORTE DE FALLO DE PARSEO PARA INSIGHT ID: {insight.get('id')}")
                print("="*80)
                print("\n[PROMPT ENVIADO AL MODELO]\n")
                print(final_prompt)
                print("\n" + "-"*80 + "\n")
                print("[RESPUESTA CRUDA RECIBIDA DEL MODELO]\n")
                print(llm_response_text)
                print("\n" + "="*80)
                print("FIN DEL REPORTE DE DEPURACIN")
                print("="*80 + "\n")
                raise ValueError("No se pudo extraer un JSON v谩lido de la respuesta del LLM.")

            # 5. Validar la estructura del JSON con Pydantic.
            # El LLM deber铆a devolver el objeto completo, incluyendo id_fragmento y fragmento_original.
            validated_output = CodingResult.model_validate(json_output)
            return validated_output

        except (ValueError, ValidationError) as e:
            self.logger.error(f"Error fatal de validaci贸n/parseo para el insight {insight.get('id')}: {e}.")
            # Relanzamos la excepci贸n para detener la ejecuci贸n (l贸gica Fail-Fast).
            # El reporte de depuraci贸n con print() ya se mostr贸 antes de este punto.
            raise e
            
        except Exception as e:
            self.logger.error(f"Error inesperado al procesar el insight {insight.get('id')}: {e}")
            raise e
