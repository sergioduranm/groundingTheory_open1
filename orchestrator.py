# orchestrator.py

import os
import json
import logging
from typing import List, Dict, Any
from dotenv import load_dotenv

from agents.coder_agent import CoderAgent
from agents.synthesizer_agent import SynthesizerAgent
from agents.categorizer_agent import CategorizerAgent
from agents.embedding_client import EmbeddingClient
from agents.codebook_repository import CodebookRepository

# Configurar un logger b√°sico para recibir los mensajes de advertencia
# (La configuraci√≥n de main.py puede sobreescribir esta, lo cual est√° bien)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class Orchestrator:
    """
    Orquesta el pipeline completo de principio a fin:
    1. Carga datos crudos.
    2. Usa el CoderAgent para generar c√≥digos abiertos.
    3. Usa el SynthesizerAgent para unificar c√≥digos y obtener un mapa de traducci√≥n.
    4. Usa el CategorizerAgent para categorizar c√≥digos en grupos conceptuales.
    5. Usa el mapa para enriquecer los datos con los IDs unificados.
    6. Guarda el resultado final.
    """
    def __init__(self):
        load_dotenv()
        google_api_key = os.getenv("GOOGLE_API_KEY")
        if not google_api_key:
            raise ValueError("La variable de entorno GOOGLE_API_KEY no est√° configurada.")

        # --- Inyecci√≥n de dependencias para los agentes ---
        self.codebook_repo = CodebookRepository(codebook_path="data/codebook.json")
        self.embedding_client = EmbeddingClient(api_key=google_api_key)
        
        self.coder = CoderAgent() 
        
        self.synthesizer = SynthesizerAgent(
            repository=self.codebook_repo, 
            client=self.embedding_client,
            similarity_threshold=0.90
        )
        
        self.categorizer = CategorizerAgent()

        # NUEVO: Definimos la ruta para los datos de entrada crudos
        self.raw_data_path = "data/data.jsonl"
        self.output_path = "data/analysis_results.jsonl" # Renombrado para reflejar el resultado final

    def _enrich_insights_with_unified_codes(
        self,
        coded_insights: List[Dict[str, Any]],
        translation_map: Dict[str, str]
    ) -> List[Dict[str, Any]]:
        """
        Toma una lista de insights codificados y un mapa de traducci√≥n,
        y devuelve una nueva lista de insights enriquecidos con los IDs de c√≥digos unificados.
        (Este m√©todo no necesita cambios, ya es perfecto)
        """
        logging.info("Enriqueciendo insights con IDs de c√≥digos unificados...")
        enriched_insights = []
        for insight in coded_insights:
            original_code_labels = insight.get("codigos_abiertos", [])
            
            unified_ids = []
            for label in original_code_labels:
                unified_id = translation_map.get(label)
                if unified_id:
                    unified_ids.append(unified_id)
                else:
                    logging.warning(
                        f"No se encontr√≥ mapeo para la etiqueta '{label}' en el insight "
                        f"'{insight.get('id_fragmento', 'ID_DESCONOCIDO')}'. "
                        "Ser√° omitida del resultado final."
                    )
            
            # Usando dictionary unpacking para una actualizaci√≥n limpia y segura
            new_insight = {**insight, "unified_code_ids": unified_ids}
            enriched_insights.append(new_insight)
        
        logging.info(f"‚úÖ {len(enriched_insights)} insights han sido enriquecidos.")
        return enriched_insights

    def run_pipeline(self):
        """
        Ejecuta el pipeline completo de orquestaci√≥n de forma autom√°tica.
        """
        logging.info("üöÄ Iniciando el pipeline de orquestaci√≥n end-to-end...")

        # --- FASE 1: Carga y Codificaci√≥n de Datos Crudos ---
        # MODIFICADO: Esta fase ahora es activa. Carga datos crudos y ejecuta el CoderAgent.
        logging.info(f"Cargando datos crudos desde {self.raw_data_path}...")
        try:
            with open(self.raw_data_path, 'r', encoding='utf-8') as f:
                raw_insights = [json.loads(line) for line in f]
            logging.info(f"Se cargaron {len(raw_insights)} insights crudos.")
        except FileNotFoundError:
            logging.error(f"FATAL: El archivo de entrada de datos crudos '{self.raw_data_path}' no existe.")
            return

        logging.info("Ejecutando CoderAgent para generar c√≥digos abiertos...")
        coded_insights = []
        all_code_labels = []
        for insight in raw_insights:
            # El CoderAgent procesa un insight y devuelve un dict con los c√≥digos
            generated_codes_dict = self.coder.generate_codes(insight)
            # Combinamos el insight original con los c√≥digos generados
            coded_insight = {**insight, **generated_codes_dict}
            coded_insights.append(coded_insight)
            all_code_labels.extend(coded_insight.get("codigos_abiertos", []))

        logging.info(f"‚úÖ CoderAgent finalizado. Se generaron un total de {len(all_code_labels)} etiquetas.")

        # --- FASE 2: S√≠ntesis de C√≥digos ---
        # (Esta fase no cambia, recibe los datos de la fase anterior)
        logging.info("Iniciando la fase de s√≠ntesis para generar el mapa de traducci√≥n...")
        translation_map = self.synthesizer.process_batch(all_code_labels)
        logging.info("‚úÖ Mapa de traducci√≥n generado exitosamente. El codebook.json ha sido actualizado.")

        # --- FASE 3: Categorizaci√≥n de C√≥digos ---
        logging.info("Iniciando la fase de categorizaci√≥n para agrupar c√≥digos conceptualmente...")
        try:
            categorized_result = self.categorizer.categorize_codes()
            self.categorizer._save_output(categorized_result)
            logging.info("‚úÖ Categorizaci√≥n completada. Las categor√≠as han sido guardadas.")
        except Exception as e:
            logging.warning(f"Error en categorizaci√≥n (continuando pipeline): {e}")

        # --- FASE 4: Enriquecimiento de Datos ---
        # (Esta fase no cambia, recibe los datos de las fases anteriores)
        enriched_data = self._enrich_insights_with_unified_codes(
            coded_insights=coded_insights,
            translation_map=translation_map
        )

        # --- FASE 5: Guardado de Resultados ---
        logging.info(f"Guardando los datos enriquecidos en '{self.output_path}'...")
        with open(self.output_path, 'w', encoding='utf-8') as f:
            for item in enriched_data:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        
        logging.info(f"üéâ ¬°Pipeline completado! El archivo de resultados '{self.output_path}' est√° listo.")
    
    def run_categorization_only(self, codes_to_categorize: List[Dict[str, Any]] = None):
        """
        Ejecuta √∫nicamente la fase de categorizaci√≥n, √∫til para an√°lisis axial
        independiente del pipeline principal.
        
        Args:
            codes_to_categorize: Lista opcional de c√≥digos espec√≠ficos a categorizar.
                               Si es None, se utilizan todos los c√≥digos del codebook.
        """
        logging.info("üîç Iniciando pipeline de categorizaci√≥n √∫nicamente...")
        
        try:
            categorized_result = self.categorizer.categorize_codes(codes_to_categorize)
            self.categorizer._save_output(categorized_result)
            logging.info("‚úÖ Categorizaci√≥n completada exitosamente.")
            return categorized_result
        except Exception as e:
            logging.error(f"‚ùå Error en el pipeline de categorizaci√≥n: {e}")
            raise